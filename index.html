<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search">
    <meta property="og:title" content="VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search" />
    <meta property="og:description" content="We propose an approach to scale up multimodal reasoing dataset" />
    <meta property="og:url" content="https://github.com/TIGER-AI-Lab/VisualWebInstruct" />
    <meta property="og:image" content="" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <title>VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Yiming Jia,
                            </span>
                            <span class="author-block">
                                 Jiachen Li,
                            </span>
                            <span class="author-block">
                                Xiang Yue,
                            </span>
                            <span class="author-block">
                                Bo Li,
                            </span>
                            <span class="author-block">
                                Ping Nie,
                            </span>
                            <span class="author-block">
                               Kai Zou,
                            </span>
                            <span class="author-block">
                               Wenhu Chen
                            </span>
                        </div>

                        

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                TIGER-Lab@University of Waterloo
                            </span>
                            <br>
                            <span class="author-block">Corresponding to:</span>
                            <span class="author-block"><a href="mailto:yiming.jia@mail.utoronto.ca">yiming.jia@mail.utoronto.ca</a>,</span>
                            <span class="author-block"><a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- GitHub link -->
                                <span class="link-block">
                                    <a href="https://github.com/TIGER-AI-Lab/VisualWebInstruct" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2503.10582" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/TIGER-Lab/MAmmoTH-VL2" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        🤗
                                      </span>
                                      <span>MAmmoTH-VL2</span>
                                  </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        🤗
                                      </span>
                                      <span>VisualWebInstruct</span>
                                  </a>
                                </span>
                                
                                <span class="link-block">
                                  <a href="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        🤗
                                      </span>
                                      <span>Seed Data</span>
                                  </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        🤗
                                      </span>
                                      <span>Recalled Data</span>
                                  </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container" style="margin-bottom: 2vh;margin-top: -6vh;">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Introduction</h2>
              <div class="content has-text-justified">
                <p>
                    Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct — a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40\% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20\% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5\% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7\%), MathVerse (42.6\%), and DynaMath (55.7\%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.                </p>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->
      </div>
      </section>

    <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 acecoder">
              🂡
            <span class="acecoder">VisualWebInstruct</span>
          </h1>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h2 class="title is-3 has-text-centered">Overview</h2>
              <div class="content has-text-justified">
                <p>VisualWebInstruct is a novel approach to create large-scale, high-quality multimodal reasoning datasets without expensive human annotation. By leveraging Google Image Search, we build a comprehensive dataset spanning multiple disciplines that enhances vision-language models' reasoning capabilities.</p>
                
                <div class="columns is-centered mt-4 mb-4">
                  <div class="column is-10">
                    <figure class="image">
                      <img src="/static/images/pipeline.png" alt="VisualWebInstruct Pipeline">
                      <figcaption class="has-text-centered">The VisualWebInstruct data generation pipeline: seed images → Google Image Search → web extraction → filtering → high-quality multimodal dataset</figcaption>
                    </figure>
                  </div>
                </div>
                
                <p>Our approach addresses a critical bottleneck in visual reasoning models by enabling the creation of diverse instruction data spanning mathematics, physics, chemistry, finance, and more.</p>
              </div>
            </div>
          </div>
        </div>
      </section>

    
      <section class="section" style="background-color: #f8f9fa;">
        <div class="container is-max-desktop">
          <h2 class="title is-3 has-text-centered">Key Results</h2>
          
          <div class="columns is-centered">
            <div class="column is-10">
              <div class="box">
                <h3 class="title is-4 has-text-centered">Performance Gains</h3>
                
                <div class="columns is-multiline">
                  <div class="column is-half">
                    <div class="notification is-primary is-light">
                      <p class="has-text-centered"><strong>MMMU-Pro (Standard)</strong></p>
                      <p class="has-text-centered title is-1">40.7%</p>
                      <p class="has-text-centered">State-of-the-art for 10B parameter models</p>
                    </div>
                  </div>
                  
                  <div class="column is-half">
                    <div class="notification is-info is-light">
                      <p class="has-text-centered"><strong>MathVista</strong></p>
                      <p class="has-text-centered title is-1">68.1%</p>
                      <p class="has-text-centered">Leading performance on mathematical visual reasoning</p>
                    </div>
                  </div>
                  
                  <div class="column is-half">
                    <div class="notification is-success is-light">
                      <p class="has-text-centered"><strong>DynaMath</strong></p>
                      <p class="has-text-centered title is-1">55.7%</p>
                      <p class="has-text-centered">Superior reasoning on dynamic math problems</p>
                    </div>
                  </div>
                  
                  <div class="column is-half">
                    <div class="notification is-warning is-light">
                      <p class="has-text-centered"><strong>Overall Improvement</strong></p>
                      <p class="has-text-centered title is-1">+5-20%</p>
                      <p class="has-text-centered">Absolute performance gain across benchmarks</p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <div class="columns is-centered mt-5">
            <div class="column is-10">
              <div class="box">
                <h3 class="title is-4 has-text-centered">Dataset Statistics</h3>
                
                <!-- 基本统计信息 -->
                <div class="columns is-multiline">
                  <div class="column is-one-third">
                    <div class="notification is-light">
                      <p class="has-text-centered"><strong>Total QA Pairs</strong></p>
                      <p class="has-text-centered title is-2">906K</p>
                    </div>
                  </div>
                  
                  <div class="column is-one-third">
                    <div class="notification is-light">
                      <p class="has-text-centered"><strong>Visual QA Pairs</strong></p>
                      <p class="has-text-centered title is-2">347K</p>
                    </div>
                  </div>
                  
                  <div class="column is-one-third">
                    <div class="notification is-light">
                      <p class="has-text-centered"><strong>Unique Images</strong></p>
                      <p class="has-text-centered title is-2">164K</p>
                    </div>
                  </div>
                </div>
                
                <!-- 主题分布 -->
                <h4 class="title is-5 has-text-centered mt-5">Subject Distribution</h4>
                <div class="table-container">
                  <table class="table is-bordered is-striped is-fullwidth">
                    <thead>
                      <tr>
                        <th>Subject</th>
                        <th>Percentage</th>
                        <th>QA Pairs</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>Mathematics</strong></td>
                        <td>62.5%</td>
                        <td>566K</td>
                      </tr>
                      <tr>
                        <td><strong>Physics</strong></td>
                        <td>14.5%</td>
                        <td>132K</td>
                      </tr>
                      <tr>
                        <td><strong>Finance</strong></td>
                        <td>7.25%</td>
                        <td>66K</td>
                      </tr>
                      <tr>
                        <td><strong>Chemistry</strong></td>
                        <td>4.8%</td>
                        <td>43K</td>
                      </tr>
                      <tr>
                        <td><strong>Engineering</strong></td>
                        <td>4.35%</td>
                        <td>39K</td>
                      </tr>
                      <tr>
                        <td><strong>Others*</strong></td>
                        <td>6.6%</td>
                        <td>60K</td>
                      </tr>
                    </tbody>
                  </table>
                  <p class="is-size-7 has-text-centered"><i>*Others includes Computer Science (2.25%), Biology (1.4%), General Knowledge (2.45%), and Humanities (0.5%)</i></p>
                </div>
              </div>
            </div>
          </div>
      </section>


      <section class="section">
        <div class="container is-max-desktop">
          <h2 class="title is-2" style="text-align: center;">Dataset Viewer</h2>
          
          <iframe 
            src="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct/embed/viewer/default/train" 
            frameborder="0" 
            width="100%" 
            height="560px">
          </iframe>
          
          <div style="text-align: center; margin-top: 20px;">
            <a href="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct" target="_blank" class="button is-primary">
              Open in Hugging Face
            </a>
          </div>
        </div>
      </section>

      



      <section class="section" style="background-color: #f8f9fa;">
        <div class="container is-max-desktop">
          <h2 class="title is-3 has-text-centered">Comprehensive Model Comparison</h2>
          
          <div class="columns is-centered">
            <div class="column is-12">
              <div class="box">
                <div class="table-container">
                  <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                    <thead>
                      <tr class="has-background-primary-light">
                        <th>Model</th>
                        <th>Size</th>
                        <th>MMMU val</th>
                        <th>MMMU-Pro std</th>
                        <th>MMMU-Pro vision</th>
                        <th>MathVista</th>
                        <th>MMVet</th>
                        <th>MathVerse</th>
                        <th>Dyna-Math</th>
                        <th>Avg</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr class="has-background-light">
                        <td colspan="10" class="has-text-centered"><strong>Closed-source Models</strong></td>
                      </tr>
                      <tr>
                        <td>GPT-4o</td>
                        <td>-</td>
                        <td>69.1</td>
                        <td>54.0</td>
                        <td>49.7</td>
                        <td>63.8</td>
                        <td>76.2</td>
                        <td>50.2</td>
                        <td>63.7</td>
                        <td>61.0</td>
                      </tr>
                      <tr>
                        <td>Gemini-1.5-Pro</td>
                        <td>-</td>
                        <td>59.1</td>
                        <td>49.4</td>
                        <td>65.8</td>
                        <td>63.9</td>
                        <td>64.0</td>
                        <td>41.2</td>
                        <td>64.8</td>
                        <td>58.3</td>
                      </tr>
                      <tr>
                        <td>Claude-3.5-Sonnet</td>
                        <td>-</td>
                        <td>68.3</td>
                        <td>55.0</td>
                        <td>48.0</td>
                        <td>67.7</td>
                        <td>75.4</td>
                        <td>44.2</td>
                        <td>60.5</td>
                        <td>59.9</td>
                      </tr>
                      <tr class="has-background-light">
                        <td colspan="10" class="has-text-centered"><strong>Open-source General Vision-Language Models</strong></td>
                      </tr>
                      <tr>
                        <td>Molmo</td>
                        <td>8B</td>
                        <td>45.3</td>
                        <td>28.3</td>
                        <td>18.9</td>
                        <td>51.6</td>
                        <td>58.0</td>
                        <td>18.9</td>
                        <td>41.6</td>
                        <td>37.5</td>
                      </tr>
                      <tr>
                        <td>Llava-OV</td>
                        <td>7B</td>
                        <td>48.8</td>
                        <td>29.5</td>
                        <td>18.7</td>
                        <td>63.2</td>
                        <td>58.6</td>
                        <td>26.2</td>
                        <td>40.3</td>
                        <td>40.8</td>
                      </tr>
                      <tr>
                        <td>Llama-3.2-Inst</td>
                        <td>11B</td>
                        <td>50.7</td>
                        <td>33.0</td>
                        <td>23.7</td>
                        <td>51.5</td>
                        <td>59.3</td>
                        <td>31.6</td>
                        <td>40.5</td>
                        <td>41.5</td>
                      </tr>
                      <tr>
                        <td>Qwen2-VL</td>
                        <td>7B</td>
                        <td>52.1</td>
                        <td>37.0</td>
                        <td>26.9</td>
                        <td>58.2</td>
                        <td>62.0</td>
                        <td>28.2</td>
                        <td>42.1</td>
                        <td>43.8</td>
                      </tr>
                      <tr>
                        <td>MAmmoTH-VL</td>
                        <td>7B</td>
                        <td>50.8</td>
                        <td>33.2</td>
                        <td>25.3</td>
                        <td>66.0</td>
                        <td>62.3</td>
                        <td>34.2</td>
                        <td>44.7</td>
                        <td>45.2</td>
                      </tr>
                      <tr>
                        <td>InternVL2.5</td>
                        <td>7B</td>
                        <td>55.8</td>
                        <td>38.2</td>
                        <td>30.4</td>
                        <td>64.4</td>
                        <td>62.8</td>
                        <td>39.5</td>
                        <td>49.8</td>
                        <td>48.7</td>
                      </tr>
                      <tr>
                        <td>Phi-4-mini</td>
                        <td>5.6B</td>
                        <td>55.1</td>
                        <td>39.7</td>
                        <td>31.2</td>
                        <td>62.4</td>
                        <td>60.5</td>
                        <td>37.6</td>
                        <td>51.4</td>
                        <td>48.6</td>
                      </tr>
                      <tr>
                        <td>DeepSeek-VL2</td>
                        <td>27B</td>
                        <td>51.1</td>
                        <td>31.4</td>
                        <td>24.3</td>
                        <td>62.8</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                      </tr>
                      <tr class="has-background-light">
                        <td colspan="10" class="has-text-centered"><strong>Specialized Reasoning Vision-Language Models</strong></td>
                      </tr>
                      <tr>
                        <td>Llava-CoT-L</td>
                        <td>11B</td>
                        <td>50.1</td>
                        <td>31.6</td>
                        <td>20.4</td>
                        <td>54.8</td>
                        <td>60.3</td>
                        <td>30.2</td>
                        <td>44.8</td>
                        <td>41.7</td>
                      </tr>
                      <tr>
                        <td>Llava-CoT-M</td>
                        <td>7B</td>
                        <td>51.4</td>
                        <td>33.0</td>
                        <td>23.7</td>
                        <td>63.8</td>
                        <td>58.6</td>
                        <td>39.4</td>
                        <td>48.3</td>
                        <td>45.5</td>
                      </tr>
                      <tr>
                        <td>LlamaV-o1</td>
                        <td>11B</td>
                        <td>49.1</td>
                        <td>31.5</td>
                        <td>22.4</td>
                        <td>54.4</td>
                        <td>63.6</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                      </tr>
                      <tr>
                        <td>Mulberry</td>
                        <td>7B</td>
                        <td>55.0</td>
                        <td>36.8</td>
                        <td>23.6</td>
                        <td>63.1</td>
                        <td>60.9</td>
                        <td>31.0</td>
                        <td>45.1</td>
                        <td>45.0</td>
                      </tr>
                      <tr>
                        <td>Insight-V</td>
                        <td>8B</td>
                        <td>50.2</td>
                        <td>30.7</td>
                        <td>20.5</td>
                        <td>59.9</td>
                        <td>60.8</td>
                        <td>28.7</td>
                        <td>47.8</td>
                        <td>42.6</td>
                      </tr>
                      <tr>
                        <td>MM-Eureka</td>
                        <td>8B</td>
                        <td>49.2</td>
                        <td>-</td>
                        <td>-</td>
                        <td>67.1</td>
                        <td>60.7</td>
                        <td>40.4</td>
                        <td>-</td>
                        <td>-</td>
                      </tr>
                      <tr class="has-background-success-light">
                        <td><strong>MAmmoTH-VL2</strong></td>
                        <td><strong>7B</strong></td>
                        <td><strong>54.7</strong></td>
                        <td><strong>40.7</strong></td>
                        <td><strong>26.3</strong></td>
                        <td><strong>68.1</strong></td>
                        <td><strong>64.5</strong></td>
                        <td><strong>42.6</strong></td>
                        <td><strong>55.7</strong></td>
                        <td><strong>50.4</strong></td>
                      </tr>
                      <tr>
                        <td>∆ over SoTA</td>
                        <td>-</td>
                        <td>-1.1</td>
                        <td>+1.0</td>
                        <td>-4.9</td>
                        <td>+2.1</td>
                        <td>+0.9</td>
                        <td>+3.1</td>
                        <td>+4.3</td>
                        <td>+1.7</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p class="has-text-centered is-italic mt-2">Evaluation results comparing MAmmoTH-VL2 with other models across multiple benchmarks. The best and second-best results across all open-source models are highlighted.</p>
                
                <div class="columns is-centered mt-5">
                  <div class="column is-10">
                    <div class="notification is-info is-light">
                      <p class="has-text-centered"><strong>Key Observations</strong></p>
                      <div class="content">
                        <ul>
                          <li>MAmmoTH-VL2 achieves state-of-the-art performance in 5 out of 7 benchmarks among all open-source models in the 7B-11B parameter range</li>
                          <li>On MMMU-Pro standard, our model shows 40.7% accuracy, outperforming all other open-source models</li>
                          <li>Particularly strong performance on mathematical reasoning benchmarks, with significant improvements on MathVerse (+3.1% over SoTA) and Dyna-Math (+4.3% over SoTA)</li>
                          <li>Overall average performance of 50.4% represents a +1.7% improvement over the previous state-of-the-art</li>
                          <li>Performance gap with closed-source models has been significantly narrowed, especially on specialized reasoning tasks</li>
                        </ul>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
    
      
    

      <section class="section" style="background-color: #f8f9fa;">
        <div class="container is-max-desktop">
          <h2 class="title is-3 has-text-centered">Research Impact</h2>
          
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <div class="notification is-primary is-light">
                  <p>VisualWebInstruct demonstrates a novel and scalable approach to creating high-quality multimodal datasets without expensive human annotation, leveraging web search technology to mine over 750,000 unique sources and construct a comprehensive dataset spanning multiple academic disciplines with diverse visual content.</p>
                </div>
                
                <div class="notification is-link is-light">
                  <p>Our approach significantly improves vision-language models' reasoning capabilities across diverse benchmarks and tasks, with MAmmoTH-VL2 achieving state-of-the-art performance (49.1% average accuracy) across seven key benchmarks including MMMU, MathVista, and Dyna-Math, demonstrating particularly strong results in mathematical reasoning with visual context.</p>
                </div>
                
                <div class="notification is-info is-light">
                  <p>The dataset's diversity in both subjects (spanning Mathematics, Physics, Chemistry, Economics, Engineering and other fields) and image types (over 163,000 unique images) creates a rich learning environment that helps models generalize to a wide range of real-world visual reasoning problems, from elementary concepts to complex college-level problems requiring multi-step deliberation.</p>
                </div>
                
                <div class="notification is-success is-light">
                  <p>MAmmoTH-VL2, our 7B-parameter model fine-tuned on VisualWebInstruct, achieves state-of-the-art performance within its parameter class on multiple benchmarks through a straightforward training approach that requires no complex methodology changes during training or inference, offering a simple yet effective solution for enhancing multimodal reasoning without the need for specialized techniques.</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>

      


    <!-- BibTeX citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Reference</h2>
            Please kindly cite our paper if you use our code or results:
            <pre><code>
                @article{visualwebinstruct,
                    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},
                    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},
                    journal={arXiv preprint arXiv:2503.10582},
                    year={2025}
                }
            </code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content has-text-centered">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
</html>
